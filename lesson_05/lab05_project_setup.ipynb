{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Automating Everything\n",
    "\n",
    "In this lab, we will learn how to setup **LLMOps with Prompt Flow** Solution Accelerator to deploy an end-to-end LLMOps solution using **Prompt Flow** and **Github Actions**.\n",
    "\n",
    "## Steps\n",
    "\n",
    "[1. Create Azure service principal](#1-create-azure-service-principal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Explicar melhor aqui o que ser√° feito\n",
    "\n",
    "### Github Workflows and Prompt Flow flows\n",
    "\n",
    "The project template comes with a few Github workflows related to three Prompt Flow example flows for providing a jumpstart.\n",
    "\n",
    "The Prompt Flow flows are **named_entity_recognition**, **web_classification** and **math_coding** each flow implementing a different use case.\n",
    "\n",
    "Each flow has 2 primary GitHub workflows and 1 optional workflow. The GitHub workflows can be found in the project .github/workflows folder.\n",
    "\n",
    "The first workflow is executed during pull request(PR) e.g. [named_entity_recognition_pr_dev_workflow.yml](../.github/workflows/named_entity_recognition_pr_dev_workflow.yml), and it helps to maintain code quality for all PRs. Usually, this pipeline uses a smaller dataset to make sure that the Prompt Flow job can be completed fast enough.\n",
    "\n",
    "The second Github workflow [named_entity_recognition_ci_dev_workflow.yml](../.github/workflows/named_entity_recognition_ci_dev_workflow.yml) is executed automatically before a PR is merged into the *development* or *main* branch. The main idea of this pipeline is to execute bulk run and evaluation on the full dataset for all prompt variants. The workflow can be modified and extended based on the project's requirements.\n",
    "\n",
    "The optional third Github workflow [named_entity_recognition_post_prod_eval.yml](../.github/workflows/named_entity_recognition_post_prod_eval.yml) need to be executed manually after the deployment of the Prompt Flow flow to production and collecting production logs (example log file - [production_log.jsonl](../named_entity_recognition/data/production_log.jsonl)). This workflow is used to evaluate the Prompt Flow flow performance in production.\n",
    "\n",
    "More details about how to create a basic Github workflows in general can be found [here](https://docs.github.com/en/actions/using-workflows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Azure service principal\n",
    "\n",
    "An Azure service principal (SP) is a special type of identity that can be used by automated tools to access Azure resources. \n",
    "\n",
    "We will use a service principal to grant GitHub Actions the permission to use the resources in our Azure subscription.\n",
    "\n",
    "Run the following bash script, updating the **spname** and **subscriptionId** variables with the values for your project to create the SP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# variables\n",
    "spname=\"LLMOps-$(date +%s)\"\n",
    "roleName=\"Owner\"\n",
    "subscriptionId=\"<subscription Id>\"\n",
    "servicePrincipalName=\"Azure-ARM-${spname}\"\n",
    "\n",
    "echo \"Using subscription ID $subscriptionID\"\n",
    "echo \"Creating SP for RBAC with name $servicePrincipalName, with role $roleName and in scopes /subscriptions/$subscriptionId\"\n",
    "az ad sp create-for-rbac --name $servicePrincipalName --role $roleName --scopes /subscriptions/$subscriptionId --sdk-auth \n",
    "echo \"Please ensure that the information created here is properly save for future use.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> The previous ```az ad create-for-rbac ...``` command will also grant the *Contributor* role to the service principal in the subscription provided. <strong>Contributor or Owner?</strong>\n",
    "\n",
    "After running the script, you'll be presented with information related to the service principal. \n",
    "\n",
    "Save this information to a safe location, you'll use it later in the GitHub configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Compute Instance and a Prompt Flow runtime\n",
    "\n",
    "Prompt Flow 'flows' require runtime associated with compute instance in Azure Machine Learning workspace. \n",
    "\n",
    "Before we run any Prompt Flow, we need to create two things using the Service Principal: a Compute Instance and a Prompt Flow runtime. \n",
    "\n",
    "This ensures that Service Principal is the owner of these resources and Flows can be executed on them from Github workflows.\n",
    "\n",
    "Update the variables with the values for your project and run the following bash script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# variables\n",
    "subscriptionId=<your azure subscription id>\n",
    "rgname=<your resource group name>\n",
    "workspace_name=<your Azure machine learning workspace name> \n",
    "userAssignedId=<enter user assigned managed identifier name>\n",
    "keyvault=<your Azure machine learning workspace associate key vault name>\n",
    "compute_name=<enter compute name>\n",
    "location=<your Azure machine learning workspace region>\n",
    "runtimeName=<enter runtime name>\n",
    "sp_id=<your azure service principal or client id>\n",
    "sp_password=<your service principal password>\n",
    "tenant_id=<your azure tenant id>\n",
    "\n",
    "echo \"> Creating user-assigned managed identity\"\n",
    "az identity create -g $rgname -n $userAssignedId --query \"id\"\n",
    "\n",
    "echo \"> Getting id, principalId of user-assigned managed identity\"\n",
    "um_details=$(az identity show -g $rgname -n $userAssignedId --query \"[id, clientId, principalId]\")\n",
    "\n",
    "echo \"> Getting id of user-assigned managed identity\"\n",
    "# user_managed_id=\"$(echo $um_details | jq -r '.[0]')\"\n",
    "user_managed_id=$(echo $um_details | sed 's/[][]//g' | awk -F'\"' '{print $2}')\n",
    "\n",
    "echo \"> Getting principal Id of user-assigned managed identity\"\n",
    "# principalId=\"$(echo $um_details | jq -r '.[2]')\"\n",
    "principalId=$(echo $um_details | sed 's/[][]//g' | awk -F'\"' '{print $6}')\n",
    "\n",
    "echo \"> Granting the user managed identity permission to access the workspace (AzureML Data Scientist)\"\n",
    "az role assignment create --assignee $principalId --role \"AzureML Data Scientist\" --scope \"/subscriptions/$subscriptionId/resourcegroups/$rgname/providers/Microsoft.MachineLearningServices/workspaces/$workspace_name\"\n",
    "\n",
    "echo \"> Granting the user managed identity permission to access the workspace keyvault (get and list)\"\n",
    "az keyvault set-policy --name $keyvault --resource-group $rgname --object-id $principalId --secret-permissions get list\n",
    "\n",
    "echo \"> Login with Service Principal\"\n",
    "az login --service-principal -u $sp_id -p $sp_password --tenant $tenant_id\n",
    "az account set -s $subscriptionId\n",
    "\n",
    "echo \"> Creating compute instance and assign user managed identity to it\"\n",
    "az ml compute create --name $compute_name --size Standard_E4s_v3 --identity-type UserAssigned --type ComputeInstance --resource-group $rgname --workspace-name $workspace_name --user-assigned-identities $user_managed_id\n",
    "\n",
    "echo \"> Getting Service Principal Azure Entra token for REST API\"\n",
    "# access_token=$(az account get-access-token | jq -r \".accessToken\")\n",
    "access_token=$(az account get-access-token | grep -oP '\"accessToken\": \"\\K[^\"]*')\n",
    "\n",
    "echo \"> Constructing POST url for runtime\"\n",
    "runtime_url_post=$(echo \"https://ml.azure.com/api/$location/flow/api/subscriptions/$subscriptionId/resourceGroups/$rgname/providers/Microsoft.MachineLearningServices/workspaces/$workspace_name/FlowRuntimes/$runtimeName?asyncCall=true\")\n",
    "\n",
    "echo \"> Constructing GET url for runtime\"\n",
    "runtime_url_get=$(echo \"https://ml.azure.com/api/$location/flow/api/subscriptions/$subscriptionId/resourceGroups/$rgname/providers/Microsoft.MachineLearningServices/workspaces/$workspace_name/FlowRuntimes/$runtimeName\")\n",
    "\n",
    "echo \"> Creating runtime using REST API\"\n",
    "curl --request POST \\\n",
    "  --url \"$runtime_url_post\" \\\n",
    "  --header \"Authorization: Bearer $access_token\" \\\n",
    "  --header 'Content-Type: application/json' \\\n",
    "  --data \"{\n",
    "    \\\"runtimeType\\\": \\\"ComputeInstance\\\",\n",
    "    \\\"computeInstanceName\\\": \\\"$compute_name\\\",\n",
    "}\"\n",
    "\n",
    "echo \"\\n> Getting runtime creation status using REST API\"\n",
    "curl --request GET \\\n",
    "  --url \"$runtime_url_get\" \\\n",
    "  --header \"Authorization: Bearer $access_token\"\n",
    "\n",
    "# Use this command multiple times unless either you get output that shows createdOn with a valid date and time value or failure. \n",
    "# In case of failure, troubleshoot the issue before moving forward.\n",
    "echo -e \"curl --request GET --url ${runtime_url_get} --header Authorization: Bearer ${access_token}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set up GitHub Repository\n",
    "\n",
    "#### 3.1. Create the repo for your project\n",
    "\n",
    "Go to GitHub and create a new repository\n",
    "\n",
    "*Github.com* > *New*\n",
    "\n",
    "<p align=\"\">\n",
    "  <img src=\"images/new_github_repo.png\" alt=\"New Github\" width=\"1024\">\n",
    "</p><BR>\n",
    "\n",
    "\n",
    "For this workshop I will use **llmops-project** as the repo name, if you chose a different repo name, you can replace llmops-project with the name you have chosen whenever you see it in the lab commands.\n",
    "\n",
    "#### 3.2. Create a GitHub personal access token\n",
    "\n",
    "Now you will create a Github **personal access token** to work with your repository from this notebook.\n",
    "\n",
    "Go to your GitHub account settings by clicking on your profile photo and then clicking on Settings and execute the following steps:\n",
    "\n",
    "1. In the left sidebar, click on Developer settings.\n",
    "2. In the left sidebar, click on Personal access tokens.\n",
    "3. Click on Generate new token (Fine-grained tokens).\n",
    "4. Choose the repo you just created in the Repository access section.\n",
    "\n",
    "<p align=\"\">\n",
    "  <img src=\"images/repository_access_token.png\" alt=\"Access token\" width=\"640\">\n",
    "</p>\n",
    "\n",
    "5. In the Permissions session add **Read and write** to the following items: **Administation**, **Contents** and **Workflows** (Metadata is mandatory).\n",
    "\n",
    "<p align=\"\">\n",
    "  <img src=\"images/repository_access_token2.png\" alt=\"Access token\" width=\"640\">\n",
    "</p><BR>\n",
    "\n",
    "6. Click on Generate token.\n",
    "\n",
    "\n",
    "Copy the token to use it in the cell bellow.\n",
    "\n",
    "#### 3.3. Populate your repo with the LLMOps project template\n",
    "\n",
    "Now you will populate your new repo with the [LLMOps Prompt Flow Template Repo](https://github.com/microsoft/llmops-promptflow-template.git).\n",
    "\n",
    "In order to do that, run the following bash script, updating the variables with the values for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# github_org=<your github organization>\n",
    "# github_repo=<your github repository>\n",
    "# your-personal-access-token=<the access token you generated>\n",
    "github_org=placerda\n",
    "github_repo=llmops-project\n",
    "your_personal_access_token=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\n",
    "\n",
    "echo \"> Cloning your project repository\"\n",
    "git clone https://${your_personal_access_token}@github.com/${github_org}/${github_repo}.git\n",
    "\n",
    "cd $github_repo\n",
    "\n",
    "echo \"> Adding a new remote named 'original'\"\n",
    "git remote add original https://github.com/microsoft/llmops-promptflow-template.git\n",
    "\n",
    "echo \"> Fetching from 'original'\"\n",
    "git fetch original main\n",
    "\n",
    "echo \"> Merging with 'original/main'\"\n",
    "git merge original/main\n",
    "\n",
    "echo \"> Pushing to 'origin main'\"\n",
    "git push origin main --repo https://${your_personal_access_token}@github.com/${github_org}/${github_repo}.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Create a development branch and make it as default\n",
    "\n",
    "Create a *development* branch from main branch and also make it as default one to make sure that all PRs should go towards it. \n",
    "\n",
    "The project template assumes that the team works at a *development* branch as a primary source for coding and improving the prompt quality. \n",
    "\n",
    "Later, you can implement Github workflows that move code from the *development* branch into qa/main or execute a release process right away. \n",
    "\n",
    "Release management is not in part of the project template.\n",
    "\n",
    "To create the *development* branch and set it as the default in the local repo, run the following bash cell after updating the variables with the values for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# github_repo=<your github repository>\n",
    "github_repo=llmops-project\n",
    "\n",
    "# Change directory to the cloned repository\n",
    "cd ${github_repo}\n",
    "\n",
    "echo \"> Creating a development branch\"\n",
    "git checkout -b development\n",
    "\n",
    "echo \"> Pushing the development branch to the remote repository\"\n",
    "git push origin development\n",
    "\n",
    "echo \"> Setting the development branch as the default branch in the local repository\"\n",
    "git remote set-head origin development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make *development* branch as default in the GitHub remote repo to make sure that all PRs should go towards it. \n",
    "\n",
    "Go to your GitHub repository on the web and execute the following steps:\n",
    "\n",
    "1. Click on the \"Settings\" tab.\n",
    "2. In the left side menu, click on \"Branches\".\n",
    "3. In the \"Default branch\" section, click the pencil icon to edit.\n",
    "4. Select the branch you want to set as default from the dropdown menu.\n",
    "5. Click \"Update\" to save your changes.\n",
    "\n",
    "Remember, changing the default branch will change the base branch for new pull requests and code reviews.\n",
    "\n",
    "<p align=\"\">\n",
    "  <img src=\"images/default_branch.png\" alt=\"Defaut branch\" width=\"1024\">\n",
    "</p>\n",
    "\n",
    "\n",
    "#### 3.5 Set up authentication with Azure and GitHub\n",
    "\n",
    "From your GitHub project, select **Settings** -> **Secrets and  variables**,  **Actions** and **New repository secret**. Create a Github repository secret named 'AZURE_CREDENTIALS' with information related to Azure Service Principal. You can paste the service principal output as the content of the secret and use [this document](https://learn.microsoft.com/en-us/azure/developer/github/connect-from-azure?tabs=azure-portal%2Clinux#use-the-azure-login-action-with-a-service-principal-secret) as a \n",
    "\n",
    "<p align=\"\">\n",
    "  <img src=\"images/github-secrets.png\" alt=\"Screenshot of GitHub Secrets\" width=\"640\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Create Azure OpenAI deployment\n",
    "\n",
    "The template project has 3 examples, all the examples use Azure OpenAI model `gpt-35-turbo` deployed with the same name `gpt-35-turbo`. \n",
    "\n",
    "Please use Azure portal [https://portal.azure.com](https://portal.azure.com) to create an Azure OpenAI resource and then to Azure OpenAI Studio [https://oai.azure.com](https://oai.azure.com) to create an Azure OpenAI gpt-35-turbo model deployment as in this example.\n",
    "\n",
    "<p align=\"\">\n",
    "  <img src=\"images/azure_oai_deployment.png\" alt=\"aoai deployment\" width=\"1024\">\n",
    "</p>\n",
    "\n",
    "### 6. Create Prompt Flow Connection\n",
    "\n",
    "Prompt Flow Connections helps securely store and manage secret keys or other sensitive credentials required for interacting with LLM and other external tools, for example Azure OpenAI.\n",
    "\n",
    "All three template project examples use a connection named `aoai` inside, we need to set up a connection with this name if we haven‚Äôt created it before.\n",
    "\n",
    "Please go to Azure Machine Learning workspace portal, click `Prompt flow` -> `Connections` -> `Create` -> `Azure OpenAI`, then follow the instructions to create your own connections called `aoai`. Learn more on [connections](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-connections?view=azureml-api-2). \n",
    "\n",
    "<p align=\"\">\n",
    "  <img src=\"images/connection.png\" alt=\"aoai connection in Prompt Flow\" width=\"1024\">\n",
    "</p>\n",
    "\n",
    "The samples use a connection named \"aoai\" connecting to a gpt-35-turbo model deployed with the same name in Azure OpenAI. This connection should be created before executing the out-of-box flows provided with the template.\n",
    "\n",
    "The configuration for connection used while authoring the repo:\n",
    "\n",
    "<p align=\"\">\n",
    "  <img src=\"images/connection-details.png\" alt=\"connection details\" width=\"640\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7. Update project configurations for Prompt flow and GitHub Actions\n",
    "\n",
    "Update code so that we can create a pull request. Update the `llmops_config.json` file for any one of the examples (e.g. `named_entity_recognization`). Update configuration so that we can create a pull request for any one of the example scenarios (e.g. named_entity_recognition). Navigate to scenario folder and update the `llmops_config.json` file. Update the KEYVAULT_NAME, RESOURCE_GROUP_NAME, RUNTIME_NAME and WORKSPACE_NAME. Update the `ENDPOINT_NAME` and `CURRENT_DEPLOYMENT_NAME` in `configs/deployment_config.json` file. Update the `ENDPOINT_NAME` and `CURRENT_DEPLOYMENT_NAME` in `configs/deployment_config.json` file.\n",
    "\n",
    "### Update llmops_config.json\n",
    "\n",
    "Modify the configuration values in the `llmops_config.json` file available for each example based on description.\n",
    "\n",
    "- `ENV_NAME`:  This represents the environment type. (The template supports *pr* and *dev* environments.)\n",
    "- `RUNTIME_NAME`:  This is the name of a Prompt Flow runtime environment, used for executing the prompt flows. Add values to this field only when you are using dedicated runtime and compute. The template uses automatic runtime by default.\n",
    "- `KEYVAULT_NAME`:  This points to an Azure Key Vault related to the Azure ML service, a service for securely storing and managing secrets, keys, and certificates.\n",
    "- `RESOURCE_GROUP_NAME`:  Name of the Azure resource group related to Azure ML workspace.\n",
    "- `WORKSPACE_NAME`:  This is name of Azure ML workspace.\n",
    "- `STANDARD_FLOW_PATH`:  This is the relative folder path to files related to a standard flow. e.g.  e.g. \"flows/standard_flow.yml\"\n",
    "- `EVALUATION_FLOW_PATH`:  This is a string value referring to relative evaluation flow paths. It can have multiple comma separated values- one for each evaluation flow. e.g. \"flows/eval_flow_1.yml,flows/eval_flow_2.yml\"\n",
    "\n",
    "For the optional post production evaluation workflow, the above configuration will be same only `ENV_NAME` will be *postprodeval* and the respective flow path need to be mentioned in `STANDARD_FLOW_PATH` configuration.\n",
    "\n",
    "### Update deployment_config.json in config folder\n",
    "\n",
    "Modify the configuration values in the `deployment_config.json` file for each environment. These are required for deploying Prompt flows in Azure ML. Ensure the values for `ENDPOINT_NAME` and `CURRENT_DEPLOYMENT_NAME` are changed before pushing the changes to remote repository.\n",
    "\n",
    "- `ENV_NAME`: This indicates the environment name, referring to the \"development\" or \"production\" or any other environment where the prompt will be deployed and used in real-world scenarios.\n",
    "- `TEST_FILE_PATH`: The value represents the file path containing sample input used for testing the deployed model.\n",
    "- `ENDPOINT_NAME`: The value represents the name or identifier of the deployed endpoint for the prompt flow.\n",
    "- `ENDPOINT_DESC`: It provides a description of the endpoint. It describes the purpose of the endpoint, which is to serve a prompt flow online.\n",
    "- `DEPLOYMENT_DESC`: It provides a description of the deployment itself.\n",
    "- `PRIOR_DEPLOYMENT_NAME`: The name of prior deployment. Used during A/B deployment. The value is \"\" if there is only a single deployment. Refer to CURRENT_DEPLOYMENT_NAME property for the first deployment.\n",
    "- `PRIOR_DEPLOYMENT_TRAFFIC_ALLOCATION`:  The traffic allocation of prior deployment. Used during A/B deployment. The value is \"\" if there is only a single deployment. Refer to CURRENT_DEPLOYMENT_TRAFFIC_ALLOCATION property for the first deployment.\n",
    "- `CURRENT_DEPLOYMENT_NAME`: The name of current deployment.\n",
    "- `CURRENT_DEPLOYMENT_TRAFFIC_ALLOCATION`: The traffic allocation of current deployment. A value of 100 indicates that all traffic is directed to this deployment.\n",
    "- `DEPLOYMENT_VM_SIZE`: This parameter specifies the size or configuration of the virtual machine instances used for the deployment.\n",
    "- `DEPLOYMENT_BASE_IMAGE_NAME`: This parameter represents the name of the base image used for creating the Prompt Flow runtime.\n",
    "-  `DEPLOYMENT_CONDA_PATH`: This parameter specifies the path to a Conda environment configuration file (usually named conda.yml), which is used to set up the deployment environment.\n",
    "- `DEPLOYMENT_INSTANCE_COUNT`:This parameter specifies the number of instances (virtual machines) that should be deployed for this particular configuration.\n",
    "- `ENVIRONMENT_VARIABLES`: This parameter represents a set of environment variables that can be passed to the deployment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
